{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jRDuJsGCgxCO"
   },
   "source": [
    "# STAT4012 Project\n",
    "\n",
    "- Implement residual network\n",
    "- 4-fold cross validation\n",
    "- Ensembling\n",
    "- Add lr_scheduler\n",
    "- Focal loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-04-11T12:41:06.291240Z",
     "iopub.status.busy": "2023-04-11T12:41:06.290545Z",
     "iopub.status.idle": "2023-04-11T12:41:07.280513Z",
     "shell.execute_reply": "2023-04-11T12:41:07.279388Z",
     "shell.execute_reply.started": "2023-04-11T12:41:06.291203Z"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1681163010779,
     "user": {
      "displayName": "Nick Wu",
      "userId": "10283763741837486654"
     },
     "user_tz": -480
    },
    "id": "VOczpMHIjDie",
    "outputId": "391dfbc4-8c44-4ad1-d5ef-972059034649"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr 11 12:41:07 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   37C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-04-11T12:41:13.145384Z",
     "iopub.status.busy": "2023-04-11T12:41:13.144840Z",
     "iopub.status.idle": "2023-04-11T12:41:13.157905Z",
     "shell.execute_reply": "2023-04-11T12:41:13.156774Z",
     "shell.execute_reply.started": "2023-04-11T12:41:13.145341Z"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1681163016035,
     "user": {
      "displayName": "Nick Wu",
      "userId": "10283763741837486654"
     },
     "user_tz": -480
    },
    "id": "cmRFCYn7ng3R",
    "outputId": "cc8e0bb9-9901-46f0-d603-3fec648ffef6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "num_cpu = mp.cpu_count()\n",
    "num_cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EVgrPb3HhJUT"
   },
   "source": [
    "# Download data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n5ceUnRihL-f"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T12:41:23.966804Z",
     "iopub.status.busy": "2023-04-11T12:41:23.966414Z",
     "iopub.status.idle": "2023-04-11T12:41:23.971223Z",
     "shell.execute_reply": "2023-04-11T12:41:23.970243Z",
     "shell.execute_reply.started": "2023-04-11T12:41:23.966772Z"
    },
    "executionInfo": {
     "elapsed": 482,
     "status": "ok",
     "timestamp": 1681163068589,
     "user": {
      "displayName": "Nick Wu",
      "userId": "10283763741837486654"
     },
     "user_tz": -480
    },
    "id": "ay3WkYnHVaVE",
    "papermill": {
     "duration": 0.0189,
     "end_time": "2022-02-23T10:03:06.279758",
     "exception": false,
     "start_time": "2022-02-23T10:03:06.260858",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "_exp_name = \"CNN4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T12:41:26.224108Z",
     "iopub.status.busy": "2023-04-11T12:41:26.223322Z",
     "iopub.status.idle": "2023-04-11T12:41:27.529474Z",
     "shell.execute_reply": "2023-04-11T12:41:27.528373Z",
     "shell.execute_reply.started": "2023-04-11T12:41:26.224062Z"
    },
    "executionInfo": {
     "elapsed": 3801,
     "status": "ok",
     "timestamp": 1681163072385,
     "user": {
      "displayName": "Nick Wu",
      "userId": "10283763741837486654"
     },
     "user_tz": -480
    },
    "id": "CwOGtRWHVaVF",
    "papermill": {
     "duration": 1.654263,
     "end_time": "2022-02-23T10:03:07.947242",
     "exception": false,
     "start_time": "2022-02-23T10:03:06.292979",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary packages.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Subset, Dataset\n",
    "from torchvision.datasets import DatasetFolder, VisionDataset\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T12:41:29.044227Z",
     "iopub.status.busy": "2023-04-11T12:41:29.043119Z",
     "iopub.status.idle": "2023-04-11T12:41:29.111363Z",
     "shell.execute_reply": "2023-04-11T12:41:29.110296Z",
     "shell.execute_reply.started": "2023-04-11T12:41:29.044187Z"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1681163072385,
     "user": {
      "displayName": "Nick Wu",
      "userId": "10283763741837486654"
     },
     "user_tz": -480
    },
    "id": "8kJm9GekVaVH",
    "papermill": {
     "duration": 0.078771,
     "end_time": "2022-02-23T10:03:08.039428",
     "exception": false,
     "start_time": "2022-02-23T10:03:07.960657",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "myseed = 4012  # set a random seed for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(myseed)\n",
    "torch.manual_seed(myseed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(myseed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d9MVtgbSVaVH",
    "papermill": {
     "duration": 0.01289,
     "end_time": "2022-02-23T10:03:08.065357",
     "exception": false,
     "start_time": "2022-02-23T10:03:08.052467",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Transforms**\n",
    "Torchvision provides lots of useful utilities for image preprocessing, data wrapping as well as data augmentation.\n",
    "\n",
    "Please refer to PyTorch official website for details about different transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T12:41:31.984491Z",
     "iopub.status.busy": "2023-04-11T12:41:31.983886Z",
     "iopub.status.idle": "2023-04-11T12:41:31.991880Z",
     "shell.execute_reply": "2023-04-11T12:41:31.990669Z",
     "shell.execute_reply.started": "2023-04-11T12:41:31.984446Z"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1681163072386,
     "user": {
      "displayName": "Nick Wu",
      "userId": "10283763741837486654"
     },
     "user_tz": -480
    },
    "id": "jvI3Xmq4VaVJ",
    "papermill": {
     "duration": 0.021406,
     "end_time": "2022-02-23T10:03:08.099437",
     "exception": false,
     "start_time": "2022-02-23T10:03:08.078031",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_tfm = transforms.Compose([\n",
    "    # (height = width = 128)\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_tfm = transforms.Compose([\n",
    "    # (height = width = 128)\n",
    "    #transforms.CenterCrop()\n",
    "    transforms.RandomResizedCrop((128, 128), scale=(0.7, 1.0)),\n",
    "    #transforms.AutoAugment(transforms.AutoAugmentPolicy.IMAGENET),\n",
    "    #transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.RandomVerticalFlip(0.5),\n",
    "    transforms.RandomRotation(180),\n",
    "    transforms.RandomAffine(30),\n",
    "    #transforms.RandomInvert(p=0.2),\n",
    "    #transforms.RandomPosterize(bits=2),\n",
    "    #transforms.RandomSolarize(threshold=192.0, p=0.2),\n",
    "    #transforms.RandomEqualize(p=0.2),\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.RandomApply(torch.nn.ModuleList([]))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D0ivMf-jVaVK",
    "papermill": {
     "duration": 0.012739,
     "end_time": "2022-02-23T10:03:08.125181",
     "exception": false,
     "start_time": "2022-02-23T10:03:08.112442",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Datasets**\n",
    "The data is labelled by the name, so we load images and label while calling '__getitem__'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T12:41:35.784394Z",
     "iopub.status.busy": "2023-04-11T12:41:35.783680Z",
     "iopub.status.idle": "2023-04-11T12:41:35.792482Z",
     "shell.execute_reply": "2023-04-11T12:41:35.791489Z",
     "shell.execute_reply.started": "2023-04-11T12:41:35.784357Z"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1681163435334,
     "user": {
      "displayName": "Nick Wu",
      "userId": "10283763741837486654"
     },
     "user_tz": -480
    },
    "id": "xBdtPhKwVaVL",
    "papermill": {
     "duration": 0.023022,
     "end_time": "2022-02-23T10:03:08.160912",
     "exception": false,
     "start_time": "2022-02-23T10:03:08.13789",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FoodDataset(Dataset):\n",
    "\n",
    "    def __init__(self,path=None,tfm=test_tfm,files=None):\n",
    "        super(FoodDataset).__init__()\n",
    "        self.path = path\n",
    "        if path:\n",
    "            self.files = sorted([os.path.join(path, x) for x in os.listdir(path) if x.endswith(\".jpg\")])\n",
    "        else:\n",
    "            self.files = files\n",
    "        self.transform = tfm\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "  \n",
    "    def __getitem__(self,idx):\n",
    "        fname = self.files[idx]\n",
    "        im = Image.open(fname)\n",
    "        im = self.transform(im)\n",
    "        #im = self.data[idx]\n",
    "        try:\n",
    "            label = int(fname.split(\"/\")[-1].split(\"_\")[0])\n",
    "        except:\n",
    "            label = -1 # test has no label\n",
    "        return im,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T12:43:00.729889Z",
     "iopub.status.busy": "2023-04-11T12:43:00.728781Z",
     "iopub.status.idle": "2023-04-11T12:43:00.747959Z",
     "shell.execute_reply": "2023-04-11T12:43:00.746833Z",
     "shell.execute_reply.started": "2023-04-11T12:43:00.729848Z"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1681163078591,
     "user": {
      "displayName": "Nick Wu",
      "userId": "10283763741837486654"
     },
     "user_tz": -480
    },
    "id": "b_kDECOJVaVL",
    "papermill": {
     "duration": 0.0258,
     "end_time": "2022-02-23T10:03:08.199437",
     "exception": false,
     "start_time": "2022-02-23T10:03:08.173637",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Residual_Block(nn.Module):\n",
    "    def __init__(self, ic, oc, stride=1):\n",
    "        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        # torch.nn.MaxPool2d(kernel_size, stride, padding)\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(ic, oc, kernel_size=3, stride=stride, padding=1),\n",
    "            nn.BatchNorm2d(oc),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(oc, oc, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(oc),\n",
    "        )\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "        self.downsample = None\n",
    "        if stride != 1 or (ic != oc):\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(ic, oc, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(oc),\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        \n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "            \n",
    "        out += residual\n",
    "        return self.relu(out)\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, block, num_layers, num_classes=11):\n",
    "        super().__init__()\n",
    "        self.preconv = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        self.layer0 = self.make_residual(block, 32, 64,  num_layers[0], stride=2)\n",
    "        self.layer1 = self.make_residual(block, 64, 128, num_layers[1], stride=2)\n",
    "        self.layer2 = self.make_residual(block, 128, 256, num_layers[2], stride=2)\n",
    "        self.layer3 = self.make_residual(block, 256, 512, num_layers[3], stride=2)\n",
    "        \n",
    "#         self.avgpool = nn.AvgPool2d(2)\n",
    "        \n",
    "        self.fc = nn.Sequential(            \n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512*4*4, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 11),\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def make_residual(self, block, ic, oc, num_layer, stride=1):\n",
    "        layers = []\n",
    "        layers.append(block(ic, oc, stride))\n",
    "        for i in range(1, num_layer):\n",
    "            layers.append(block(oc, oc))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # [3, 128, 128]\n",
    "        out = self.preconv(x)  # [32, 64, 64]\n",
    "        out = self.layer0(out) # [64, 32, 32]\n",
    "        out = self.layer1(out) # [128, 16, 16]\n",
    "        out = self.layer2(out) # [256, 8, 8]\n",
    "        out = self.layer3(out) # [512, 4, 4]\n",
    "#         out = self.avgpool(out) # [512, 2, 2]\n",
    "        out = self.fc(out.view(out.size(0), -1)) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, class_num, alpha=None, gamma=2, size_average=True):\n",
    "        super().__init__()\n",
    "        if alpha is None:\n",
    "            self.alpha = Variable(torch.ones(class_num, 1))\n",
    "        else:\n",
    "            if isinstance(alpha, Variable):\n",
    "                self.alpha = alpha\n",
    "            else:\n",
    "                self.alpha = Variable(alpha)\n",
    "        self.gamma = gamma\n",
    "        self.class_num = class_num\n",
    "        self.size_average = size_average\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        N = inputs.size(0)\n",
    "        C = inputs.size(1)\n",
    "        P = F.softmax(inputs, dim=1)\n",
    "        \n",
    "        class_mask = inputs.data.new(N, C).fill_(0)\n",
    "        class_mask = Variable(class_mask)\n",
    "        ids = targets.view(-1, 1)\n",
    "        class_mask.scatter_(1, ids.data, 1.)\n",
    "        \n",
    "        if inputs.is_cuda and not self.alpha.is_cuda:\n",
    "            self.alpha = self.alpha.cuda()\n",
    "        alpha = self.alpha[ids.data.view(-1)]\n",
    "        probs = (P*class_mask).sum(1).view(-1, 1)\n",
    "        \n",
    "        log_p = probs.log()\n",
    "        \n",
    "        batch_loss = -alpha*(torch.pow((1-probs), self.gamma))*log_p\n",
    "        \n",
    "        if self.size_average:\n",
    "            loss = batch_loss.mean()\n",
    "        else:\n",
    "            loss = batch_loss.sum()\n",
    "            \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T12:43:06.183667Z",
     "iopub.status.busy": "2023-04-11T12:43:06.182615Z",
     "iopub.status.idle": "2023-04-11T12:43:06.190014Z",
     "shell.execute_reply": "2023-04-11T12:43:06.188866Z",
     "shell.execute_reply.started": "2023-04-11T12:43:06.183592Z"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1681163090164,
     "user": {
      "displayName": "Nick Wu",
      "userId": "10283763741837486654"
     },
     "user_tz": -480
    },
    "id": "raSJc5-bCtqV"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_layers = [2, 3, 3, 1] # residual number layers\n",
    "# （0, 994), (1, 429), (2, 1500), (3, 986), (4, 848), (5, 1325)\n",
    "#  (6, 440), (7, 280), (8, 855), (9, 1500), (10, 709)\n",
    "alpha = torch.Tensor([1.4, 3.2, 1, 1.4, 1.5, 1, 3.2, 4.5, 1.5, 1, 2])\n",
    "\n",
    "n_epochs = 300\n",
    "patience = 20 # If no improvement in 'patience' epochs, early stop\n",
    "\n",
    "k_fold = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-11T12:43:09.734189Z",
     "iopub.status.busy": "2023-04-11T12:43:09.733174Z",
     "iopub.status.idle": "2023-04-11T12:43:09.778245Z",
     "shell.execute_reply": "2023-04-11T12:43:09.777168Z",
     "shell.execute_reply.started": "2023-04-11T12:43:09.734136Z"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1681163295457,
     "user": {
      "displayName": "Nick Wu",
      "userId": "10283763741837486654"
     },
     "user_tz": -480
    },
    "id": "2_OeWtstVaVO",
    "papermill": {
     "duration": 0.054295,
     "end_time": "2022-02-23T10:03:08.266338",
     "exception": false,
     "start_time": "2022-02-23T10:03:08.212043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13296"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir = \"food-11/training\"\n",
    "val_dir = \"food-11/validation\"\n",
    "\n",
    "train_files = [os.path.join(train_dir, x) for x in os.listdir(train_dir) if x.endswith('.jpg')]\n",
    "val_files = [os.path.join(val_dir, x) for x in os.listdir(val_dir) if x.endswith('.jpg')]\n",
    "total_files = train_files + val_files\n",
    "random.seed(myseed)\n",
    "random.shuffle(total_files)\n",
    "\n",
    "num = len(total_files) // k_fold\n",
    "len(total_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-04-11T12:43:12.357181Z",
     "iopub.status.busy": "2023-04-11T12:43:12.356757Z"
    },
    "id": "Lp3y_ITOEUN_",
    "outputId": "3d1b7e03-d1bc-4c0f-abce-26666362f789"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "\n",
      "\n",
      "Starting Fold: 1 ********************************************\n",
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T: 001/300: 100%|██████████| 78/78 [01:33<00:00,  1.19s/it, lr=0.0004, b_loss=1.95, b_acc=0.319, loss=2.12, acc=0.247] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 001/300 ] loss = 2.11805, acc = 0.24658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "V: 001/300: 100%|██████████| 26/26 [00:27<00:00,  1.07s/it, v_loss=2.03, v_acc=0.299]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 001/300 ] loss = 2.03091, acc = 0.29854\n",
      "[ Valid | 001/300 ] loss = 2.03091, acc = 0.29854 -> best\n",
      "Best model found at fold 1 epoch 1, acc=0.29854, saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T: 002/300: 100%|██████████| 78/78 [01:07<00:00,  1.16it/s, lr=0.000398, b_loss=1.81, b_acc=0.414, loss=1.94, acc=0.306]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 002/300 ] loss = 1.94454, acc = 0.30569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "V: 002/300: 100%|██████████| 26/26 [00:19<00:00,  1.34it/s, v_loss=1.82, v_acc=0.367]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 002/300 ] loss = 1.81855, acc = 0.36651\n",
      "[ Valid | 002/300 ] loss = 1.81855, acc = 0.36651 -> best\n",
      "Best model found at fold 1 epoch 2, acc=0.36651, saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T: 003/300: 100%|██████████| 78/78 [01:05<00:00,  1.18it/s, lr=0.00039, b_loss=2.01, b_acc=0.302, loss=1.88, acc=0.338]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 003/300 ] loss = 1.88497, acc = 0.33820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "V: 003/300: 100%|██████████| 26/26 [00:19<00:00,  1.32it/s, v_loss=2.12, v_acc=0.298]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 003/300 ] loss = 2.12169, acc = 0.29815\n",
      "[ Valid | 003/300 ] loss = 2.12169, acc = 0.29815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T: 004/300: 100%|██████████| 78/78 [01:07<00:00,  1.16it/s, lr=0.000378, b_loss=1.75, b_acc=0.397, loss=1.79, acc=0.369]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 004/300 ] loss = 1.79490, acc = 0.36857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "V: 004/300: 100%|██████████| 26/26 [00:18<00:00,  1.38it/s, v_loss=2.06, v_acc=0.321]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 004/300 ] loss = 2.05865, acc = 0.32090\n",
      "[ Valid | 004/300 ] loss = 2.05865, acc = 0.32090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T: 005/300: 100%|██████████| 78/78 [01:07<00:00,  1.16it/s, lr=0.000362, b_loss=1.82, b_acc=0.405, loss=1.71, acc=0.401]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 005/300 ] loss = 1.70568, acc = 0.40063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "V: 005/300: 100%|██████████| 26/26 [00:19<00:00,  1.32it/s, v_loss=1.76, v_acc=0.407]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 005/300 ] loss = 1.75774, acc = 0.40674\n",
      "[ Valid | 005/300 ] loss = 1.75774, acc = 0.40674 -> best\n",
      "Best model found at fold 1 epoch 5, acc=0.40674, saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T: 006/300: 100%|██████████| 78/78 [01:06<00:00,  1.17it/s, lr=0.000341, b_loss=1.74, b_acc=0.388, loss=1.65, acc=0.427]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 006/300 ] loss = 1.65336, acc = 0.42735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "V: 006/300: 100%|██████████| 26/26 [00:19<00:00,  1.32it/s, v_loss=1.63, v_acc=0.442]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 006/300 ] loss = 1.63194, acc = 0.44161\n",
      "[ Valid | 006/300 ] loss = 1.63194, acc = 0.44161 -> best\n",
      "Best model found at fold 1 epoch 6, acc=0.44161, saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T: 007/300: 100%|██████████| 78/78 [01:07<00:00,  1.16it/s, lr=0.000318, b_loss=1.49, b_acc=0.491, loss=1.59, acc=0.446]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 007/300 ] loss = 1.58626, acc = 0.44640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "V: 007/300: 100%|██████████| 26/26 [00:18<00:00,  1.37it/s, v_loss=1.58, v_acc=0.453]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 007/300 ] loss = 1.58459, acc = 0.45334\n",
      "[ Valid | 007/300 ] loss = 1.58459, acc = 0.45334 -> best\n",
      "Best model found at fold 1 epoch 7, acc=0.45334, saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T: 008/300: 100%|██████████| 78/78 [01:07<00:00,  1.15it/s, lr=0.000291, b_loss=1.53, b_acc=0.526, loss=1.51, acc=0.476]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 008/300 ] loss = 1.51147, acc = 0.47619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "V: 008/300: 100%|██████████| 26/26 [00:18<00:00,  1.38it/s, v_loss=1.59, v_acc=0.452]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 008/300 ] loss = 1.59191, acc = 0.45155\n",
      "[ Valid | 008/300 ] loss = 1.59191, acc = 0.45155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T: 009/300: 100%|██████████| 78/78 [01:06<00:00,  1.17it/s, lr=0.000262, b_loss=1.51, b_acc=0.44, loss=1.47, acc=0.494] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 009/300 ] loss = 1.46563, acc = 0.49412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "V: 009/300: 100%|██████████| 26/26 [00:19<00:00,  1.33it/s, v_loss=1.54, v_acc=0.466]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 009/300 ] loss = 1.54291, acc = 0.46638\n",
      "[ Valid | 009/300 ] loss = 1.54291, acc = 0.46638 -> best\n",
      "Best model found at fold 1 epoch 9, acc=0.46638, saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T: 010/300: 100%|██████████| 78/78 [01:06<00:00,  1.18it/s, lr=0.000231, b_loss=1.34, b_acc=0.474, loss=1.42, acc=0.51] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 010/300 ] loss = 1.41721, acc = 0.50988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "V: 010/300: 100%|██████████| 26/26 [00:20<00:00,  1.29it/s, v_loss=1.44, v_acc=0.497]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 010/300 ] loss = 1.44383, acc = 0.49665\n",
      "[ Valid | 010/300 ] loss = 1.44383, acc = 0.49665 -> best\n",
      "Best model found at fold 1 epoch 10, acc=0.49665, saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T: 011/300: 100%|██████████| 78/78 [01:05<00:00,  1.19it/s, lr=0.0002, b_loss=1.25, b_acc=0.569, loss=1.37, acc=0.534]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 011/300 ] loss = 1.36530, acc = 0.53414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "V: 011/300: 100%|██████████| 26/26 [00:18<00:00,  1.40it/s, v_loss=1.56, v_acc=0.476]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 011/300 ] loss = 1.56298, acc = 0.47595\n",
      "[ Valid | 011/300 ] loss = 1.56298, acc = 0.47595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T: 012/300: 100%|██████████| 78/78 [01:05<00:00,  1.19it/s, lr=0.000169, b_loss=1.37, b_acc=0.543, loss=1.32, acc=0.549]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 012/300 ] loss = 1.31890, acc = 0.54883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "V: 012/300: 100%|██████████| 26/26 [00:19<00:00,  1.33it/s, v_loss=1.24, v_acc=0.567]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 012/300 ] loss = 1.23980, acc = 0.56741\n",
      "[ Valid | 012/300 ] loss = 1.23980, acc = 0.56741 -> best\n",
      "Best model found at fold 1 epoch 12, acc=0.56741, saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T: 013/300:  96%|█████████▌| 75/78 [01:03<00:02,  1.32it/s, lr=0.000138, b_loss=1.44, b_acc=0.547, loss=1.27, acc=0.555]"
     ]
    }
   ],
   "source": [
    "# \"cuda\" only when GPUs are available.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "test_fold = k_fold\n",
    "\n",
    "for i in range(test_fold):\n",
    "    fold = i+1\n",
    "    print(f'\\n\\nStarting Fold: {fold} ********************************************')\n",
    "    model = Classifier(Residual_Block, num_layers).to(device)\n",
    "    print(next(model.parameters()).device)\n",
    "    criterion = FocalLoss(11, alpha=alpha)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0004, weight_decay=1e-5) \n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=20, T_mult=1)\n",
    "    stale = 0\n",
    "    best_acc = 0\n",
    "    \n",
    "    val_data = total_files[i*num: (i+1)*num]\n",
    "    train_data = total_files[:i*num] + total_files[(i+1)*num:]\n",
    "    \n",
    "    train_set = FoodDataset(tfm=train_tfm, files=train_data)\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_cpu, pin_memory=True)\n",
    "    \n",
    "    valid_set = FoodDataset(tfm=test_tfm, files=val_data)\n",
    "    valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=num_cpu, pin_memory=True)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "    \n",
    "        # ---------- Training ----------\n",
    "        # Make sure the model is in train mode before training.\n",
    "        model.train()\n",
    "    \n",
    "        # These are used to record information in training.\n",
    "        train_loss = []\n",
    "        train_accs = []\n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        \n",
    "        pbar = tqdm(train_loader)\n",
    "        pbar.set_description(f'T: {epoch+1:03d}/{n_epochs:03d}')\n",
    "        for batch in pbar:\n",
    "    \n",
    "            # A batch consists of image data and corresponding labels.\n",
    "            imgs, labels = batch\n",
    "            #imgs = imgs.half()\n",
    "            #print(imgs.shape,labels.shape)\n",
    "    \n",
    "            # Forward the data. (Make sure data and model are on the same device.)\n",
    "            logits = model(imgs.to(device))\n",
    "    \n",
    "            # Calculate the cross-entropy loss.\n",
    "            # We don't need to apply softmax before computing cross-entropy as it is done automatically.\n",
    "            loss = criterion(logits, labels.to(device))\n",
    "    \n",
    "            # Gradients stored in the parameters in the previous step should be cleared out first.\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            # Compute the gradients for parameters.\n",
    "            loss.backward()\n",
    "    \n",
    "            # Clip the gradient norms for stable training.\n",
    "            grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "    \n",
    "            # Update the parameters with computed gradients.\n",
    "            optimizer.step()\n",
    "    \n",
    "            # Compute the accuracy for current batch.\n",
    "            acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n",
    "    \n",
    "            # Record the loss and accuracy.\n",
    "            train_loss.append(loss.item())\n",
    "            train_accs.append(acc)\n",
    "            pbar.set_postfix({'lr':lr, 'b_loss':loss.item(), 'b_acc':acc.item(),\n",
    "                    'loss':sum(train_loss)/len(train_loss), 'acc': sum(train_accs).item()/len(train_accs)})\n",
    "            \n",
    "        train_loss = sum(train_loss) / len(train_loss)\n",
    "        train_acc = sum(train_accs) / len(train_accs)\n",
    "        # Print the information.\n",
    "        print(f\"[ Train | {epoch + 1:03d}/{n_epochs:03d} ] loss = {train_loss:.5f}, acc = {train_acc:.5f}\")\n",
    "        scheduler.step()\n",
    "        \n",
    "        \n",
    "        # Make sure the model is in eval mode so that some modules like dropout are disabled and work normally.\n",
    "        model.eval()\n",
    "    \n",
    "        # These are used to record information in validation.\n",
    "        valid_loss = []\n",
    "        valid_accs = []\n",
    "    \n",
    "        # Iterate the validation set by batches.\n",
    "        pbar = tqdm(valid_loader)\n",
    "        pbar.set_description(f'V: {epoch+1:03d}/{n_epochs:03d}')\n",
    "        for batch in pbar:\n",
    "\n",
    "            # A batch consists of image data and corresponding labels.\n",
    "            imgs, labels = batch\n",
    "            #imgs = imgs.half()\n",
    "    \n",
    "            # We don't need gradient in validation.\n",
    "            # Using torch.no_grad() accelerates the forward process.\n",
    "            with torch.no_grad():\n",
    "                logits = model(imgs.to(device))\n",
    "    \n",
    "            # We can still compute the loss (but not the gradient).\n",
    "            loss = criterion(logits, labels.to(device))\n",
    "    \n",
    "            # Compute the accuracy for current batch.\n",
    "            acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n",
    "    \n",
    "            # Record the loss and accuracy.\n",
    "            valid_loss.append(loss.item())\n",
    "            valid_accs.append(acc)\n",
    "            pbar.set_postfix({'v_loss':sum(valid_loss)/len(valid_loss), \n",
    "                              'v_acc': sum(valid_accs).item()/len(valid_accs)})\n",
    "        \n",
    "    \n",
    "        # The average loss and accuracy for entire validation set is the average of the recorded values.\n",
    "        valid_loss = sum(valid_loss) / len(valid_loss)\n",
    "        valid_acc = sum(valid_accs) / len(valid_accs)\n",
    "\n",
    "        # Print the information.\n",
    "        print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\")\n",
    "\n",
    "        # update logs\n",
    "        if valid_acc > best_acc:\n",
    "            with open(f\"{_exp_name}_fold_{fold}_log.txt\",\"a\") as f:\n",
    "                newline = '\\n'\n",
    "                item = f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f} -> best{newline}\"\n",
    "                f.write(item)\n",
    "                print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f} -> best\")\n",
    "        else:\n",
    "            with open(f\"{_exp_name}_fold_{fold}_log.txt\",\"a\") as f:\n",
    "                newline = '\\n'\n",
    "                item = f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}{newline}\"\n",
    "                f.write(item)\n",
    "                print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\")\n",
    "    \n",
    "        # save models\n",
    "        if valid_acc > best_acc:\n",
    "            print(f\"Best model found at fold {fold} epoch {epoch+1}, acc={valid_acc:.5f}, saving model\")\n",
    "            torch.save(model.state_dict(), f\"Fold_{fold}_best.ckpt\")\n",
    "            # only save best to prevent output memory exceed error\n",
    "            best_acc = valid_acc\n",
    "            stale = 0\n",
    "        else:\n",
    "            stale += 1\n",
    "            if stale > patience:\n",
    "                print(f\"No improvment {patience} consecutive epochs, early stopping\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1681160711111,
     "user": {
      "displayName": "Nick Wu",
      "userId": "10283763741837486654"
     },
     "user_tz": -480
    },
    "id": "B9QNdHIXVaVP",
    "outputId": "0d0024e1-07b4-498f-9a3b-ea1815319f95",
    "papermill": {
     "duration": 0.493644,
     "end_time": "2022-02-23T19:10:19.985992",
     "exception": false,
     "start_time": "2022-02-23T19:10:19.492348",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dir = \"food-11/test\"\n",
    "test_set = FoodDataset(test_dir, tfm=test_tfm)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=num_cpu, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G31uyjpvVaVP",
    "papermill": {
     "duration": 0.498773,
     "end_time": "2022-02-23T19:10:20.961802",
     "exception": false,
     "start_time": "2022-02-23T19:10:20.463029",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Testing and generate prediction CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6246,
     "status": "ok",
     "timestamp": 1681160717352,
     "user": {
      "displayName": "Nick Wu",
      "userId": "10283763741837486654"
     },
     "user_tz": -480
    },
    "id": "bpLtxx5FVaVP",
    "outputId": "bf66ac1c-27e3-4097-ac23-457dc0b7ba36",
    "papermill": {
     "duration": 49.157727,
     "end_time": "2022-02-23T19:11:10.61523",
     "exception": false,
     "start_time": "2022-02-23T19:10:21.457503",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "for i in range(test_fold):\n",
    "    fold = i + 1\n",
    "    model_best = Classifier(Residual_Block, num_layers).to(device)\n",
    "    model_best.load_state_dict(torch.load(f\"Fold_{fold}_best.ckpt\"))\n",
    "    model_best.eval()\n",
    "    models.append(model_best)\n",
    "\n",
    "prediction = []            \n",
    "with torch.no_grad():\n",
    "    for data,_ in test_loader:\n",
    "        test_preds = [] \n",
    "        for model_best in models:\n",
    "            test_preds.append(model_best(data.to(device)).cpu().data.numpy())\n",
    "        test_preds = sum(test_preds)\n",
    "        test_label = np.argmax(test_preds, axis=1)\n",
    "        prediction += test_label.squeeze().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fKupB3VUVaVQ",
    "papermill": {
     "duration": 0.554276,
     "end_time": "2022-02-23T19:11:11.870035",
     "exception": false,
     "start_time": "2022-02-23T19:11:11.315759",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create test csv\n",
    "def pad4(i):\n",
    "    return \"0\"*(4-len(str(i)))+str(i)\n",
    "df = pd.DataFrame()\n",
    "df[\"Id\"] = [pad4(i) for i in range(1,len(test_set)+1)]\n",
    "df[\"Category\"] = prediction\n",
    "df.to_csv(\"prediction_CNN3.csv\",index = False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "15hMu9YiYjE_6HY99UXon2vKGk2KwugWu",
     "timestamp": 1681142933591
    }
   ]
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
